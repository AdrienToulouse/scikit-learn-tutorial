# ---
# jupyter:
#   jupytext:
#     formats: notebooks//ipynb,markdown_files//md,python_scripts//py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.2'
#       jupytext_version: 1.1.7
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %%
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

# %% [markdown]
# # Supervised Learning Part 1 -- Classification

# %% [markdown]
# To visualize the workings of machine learning algorithms, it is often helpful to study two-dimensional or one-dimensional data, that is data with only one or two features. While in practice, datasets usually have many more features, it is hard to plot high-dimensional data in on two-dimensional screens.
#
# We will illustrate some very simple examples before we move on to more "real world" data sets.
#

# %% [markdown]
# First, we will look at a two class classification problem in two dimensions. We use the synthetic data generated by the ``make_blobs`` function.

# %%
from sklearn.datasets import make_blobs

X, y = make_blobs(centers=2, random_state=0, cluster_std=1.5)

print('X ~ n_samples x n_features:', X.shape)
print('y ~ n_samples:', y.shape)

# %%
print('First 5 samples:\n', X[:5, :])

# %%
print('First 5 labels:', y[:5])

# %% [markdown]
# As the data is two-dimensional, we can plot each sample as a point in a two-dimensional coordinate system, with the first feature being the x-axis and the second feature being the y-axis.

# %%
plt.figure(figsize=(8, 8))
plt.scatter(X[y == 0, 0], X[y == 0, 1], s=40, label='0')
plt.scatter(X[y == 1, 0], X[y == 1, 1], s=40, label='1',
            marker='s')

plt.xlabel('first feature')
plt.ylabel('second feature')
plt.legend(loc='upper right');

# %% [markdown]
# Classification is a supervised task, and since we are interested in its performance on unseen data, we split our data into two parts:
#
# 1. a training set that the learning algorithm uses to fit the model
# 2. a test set to evaluate the generalization performance of the model
#
# The ``train_test_split`` function from the ``model_selection`` module does that for us -- we will use it to split a dataset into 75% training data and 25% test data.
#
# <img src="figures/train_test_split_matrix.svg" width="100%">
#

# %%
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.25,
                                                    random_state=1234,
                                                    stratify=y)

# %% [markdown]
# ### The scikit-learn estimator API
# <img src="figures/supervised_workflow.svg" width="100%">
#

# %% [markdown]
# Every algorithm is exposed in scikit-learn via an ''Estimator'' object. (All models in scikit-learn have a very consistent interface). For instance, we first import the logistic regression class.

# %%
from sklearn.linear_model import LogisticRegression

# %% [markdown]
# Next, we instantiate the estimator object.

# %%
classifier = LogisticRegression()

# %%
X_train.shape

# %%
y_train.shape

# %% [markdown]
# To built the model from our data, that is to learn how to classify new points, we call the ``fit`` function with the training data, and the corresponding training labels (the desired output for the training data point):

# %%
classifier.fit(X_train, y_train)

# %% [markdown]
# (Some estimator methods such as `fit` return `self` by default. Thus, after executing the code snippet above, you will see the default parameters of this particular instance of `LogisticRegression`. Another way of retrieving the estimator's ininitialization parameters is to execute `classifier.get_params()`, which returns a parameter dictionary.)

# %% [markdown]
# We can then apply the model to unseen data and use the model to predict the estimated outcome using the ``predict`` method:

# %%
prediction = classifier.predict(X_test)

# %% [markdown]
# We can compare these against the true labels:

# %%
print(prediction)
print(y_test)

# %% [markdown]
# We can evaluate our classifier quantitatively by measuring what fraction of predictions is correct. This is called **accuracy**:

# %%
np.mean(prediction == y_test)

# %% [markdown]
# There is also a convenience function , ``score``, that all scikit-learn classifiers have to compute this directly from the test data:
#     

# %%
classifier.score(X_test, y_test)

# %% [markdown]
# It is often helpful to compare the generalization performance (on the test set) to the performance on the training set:

# %%
classifier.score(X_train, y_train)

# %% [markdown]
# LogisticRegression is a so-called linear model,
# that means it will create a decision that is linear in the input space. In 2d, this simply means it finds a line to separate the blue from the red:

# %%
from figures import plot_2d_separator

plt.scatter(X[y == 0, 0], X[y == 0, 1], s=40, label='0')
plt.scatter(X[y == 1, 0], X[y == 1, 1], s=40, label='1', marker='s')

plt.xlabel("first feature")
plt.ylabel("second feature")
plot_2d_separator(classifier, X)
plt.legend(loc='upper right');

# %% [markdown]
# **Estimated parameters**: All the estimated model parameters are attributes of the estimator object ending by an underscore. Here, these are the coefficients and the offset of the line:

# %%
print(classifier.coef_)
print(classifier.intercept_)

# %% [markdown]
# Another classifier: K Nearest Neighbors
# ------------------------------------------------
# Another popular and easy to understand classifier is K nearest neighbors (kNN).  It has one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class.
#
# The interface is exactly the same as for ``LogisticRegression above``.

# %%
from sklearn.neighbors import KNeighborsClassifier

# %% [markdown]
# This time we set a parameter of the KNeighborsClassifier to tell it we only want to look at one nearest neighbor:

# %%
knn = KNeighborsClassifier(n_neighbors=30)

# %% [markdown]
# We fit the model with out training data

# %%
knn.fit(X_train, y_train)

# %%
plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],
            s=40, label='0')
plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],
            s=40, label='1', marker='s')

plt.xlabel("first feature")
plt.ylabel("second feature")
plot_2d_separator(knn, X)
plt.legend(loc='upper right');

# %%
knn.score(X_train, y_train)

# %%
plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1],
            s=40, label='0')
plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1],
            s=40, label='1', marker='s')

plt.xlabel("first feature")
plt.ylabel("second feature")
plot_2d_separator(knn, X)
plt.legend(loc='upper right');

# %%
knn.score(X_test, y_test)

# %% [markdown]
# <div class="alert alert-success">
#     <b>EXERCISE</b>:
#      <ul>
#       <li>
#       Apply the KNeighborsClassifier to the ``iris`` dataset. Play with different values of the ``n_neighbors`` and observe how training and test score change.
#       </li>
#     </ul>
# </div>

# %%

# %%
# # %load solutions/05A_knn_with_diff_k.py
