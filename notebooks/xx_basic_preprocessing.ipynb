{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to scikit-learn: basic preprocessing for basic model fitting\n",
    "\n",
    "In this notebook, we will aim at introducing a typical approach to build\n",
    "predictive models on tabular datasets.\n",
    "\n",
    "In particular we will highlight:\n",
    "* the difference between numerical and categorical variables;\n",
    "* the importance of scaling numerical variables;\n",
    "* typical ways to deal categorical variables;\n",
    "* train predictive models on different kinds of data;\n",
    "* evaluate the performance of a model via cross-validation.\n",
    "\n",
    "## Introduce the dataset\n",
    "\n",
    "To this aim, we will use data from the 1994 Census bureau database. The goal\n",
    "with this data is to regress wages from heterogeneous data such as age,\n",
    "employment, education, family information, etc.\n",
    "\n",
    "Let's first load the data located in the `datasets` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/adult-census.csv')\n",
    "# df = pd.read_csv(\"https://www.openml.org/data/get_csv/1595261/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's have a look at the first records of this data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The target variable in our study will be the \"class\" column while we will use\n",
    "the other columns as input variables for our model. This target column divides\n",
    "the samples (also known as records) into two groups: high income (>50K) vs low\n",
    "income (<=50K). The resulting prediction problem is therefore a binary\n",
    "classification problem.\n",
    "\n",
    "For simplicity, we will ignore the \"fnlwgt\" (final weight) column that was\n",
    "crafted by the creators of the dataset when sampling the dataset to be\n",
    "representative of the full census database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = df[target_name].to_numpy()\n",
    "data = df.drop(columns=[target_name, \"fnlwgt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can check the number of samples and the number of features available in\n",
    "the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The dataset contains {data.shape[0]} samples and {data.shape[1]} \"\n",
    "    \"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Working with numerical data\n",
    "\n",
    "The numerical data is the most natural type of data used in machine learning\n",
    "and can (almost) directly be fed to predictive models. We can quickly have a\n",
    "look at such data by selecting the subset of columns from the original data.\n",
    "\n",
    "We will use this subset of data to fit a linear classification model to\n",
    "predict the income class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['age', 'education-num', 'hours-per-week',\n",
    "                     'capital-gain', 'capital-loss']\n",
    "data_numeric = data[numerical_columns]\n",
    "data_numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a machine learning model, it is important to leave out a\n",
    "subset of the data which we can use later to evaluate the trained model.\n",
    "The data used to fit a model a called training data while the one used to\n",
    "assess a model are called testing data.\n",
    "\n",
    "Scikit-learn provides an helper function `train_test_split` which will\n",
    "split the dataset into a training and a testing set. It will ensure that\n",
    "the data are shuffled randomly before splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data_numeric, target, random_state=42\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The training dataset contains {data_train.shape[0]} samples and \"\n",
    "    f\"{data_train.shape[1]} features\"\n",
    ")\n",
    "print(\n",
    "    f\"The testing dataset contains {data_test.shape[0]} samples and \"\n",
    "    f\"{data_test.shape[1]} features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "We will build a linear classification model called \"Logistic Regression\". The\n",
    "`fit` method is called to train the model from the input and target data. Only\n",
    "the training data should be given for this purpose.\n",
    "\n",
    "In addition, when checking the time required to train the model and internally\n",
    "check the number of iterations done by the solver to find a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "model = LogisticRegression()\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start\n",
    "\n",
    "print(\n",
    "    f\"The model {model.__class__.__name__} was trained in \"\n",
    "    f\"{elapsed_time:.3f} seconds for {model.n_iter_} iterations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's ignore the convergence warning for now and instead let's try\n",
    "to use our model to make some predictions on the first three records\n",
    "of the held out test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.predict(data_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To quantitatively evaluate our model, we can use the method `score`. It will\n",
    "compute the classification accuracy when dealing with a classificiation\n",
    "problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The test accuracy using a {model.__class__.__name__} is \"\n",
    "    f\"{model.score(data_test, target_test):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's now consider the `ConvergenceWarning` message that was raised previously\n",
    "when calling the `fit` method to train our model. This warning informs us that\n",
    "our model stopped learning becaused it reached the maximum number of\n",
    "iterations allowed by the user. This could potentially be detrimental for the\n",
    "model accuracy. We can follow the (bad) advice given in the warning message\n",
    "and increase the maximum number of iterations allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=50000)\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start\n",
    "print(\n",
    "    f\"The accuracy using a {model.__class__.__name__} is \"\n",
    "    f\"{model.score(data_test, target_test):.3f} with a fitting time of \"\n",
    "    f\"{elapsed_time:.3f} seconds in {model.n_iter_} iterations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can observe now a longer training time but not significant improvement in\n",
    "the predictive performance. Instead of increasing the number of iterations, we\n",
    "can try to help fit the model faster by scaling the data first. A range of\n",
    "preprocessing algorithms in scikit-learn allows to transform the input data\n",
    "before training a model. We can easily combine these sequential operation with\n",
    "a scikit-learn `Pipeline` which will chain the operations and can be used as\n",
    "any other classifier or regressor. The helper function `make_pipeline` will\n",
    "create a `Pipeline` by giving the successive transformations to perform.\n",
    "\n",
    "In our case, we will standardize the data and then train a new logistic\n",
    "regression model on that new version of the dataset set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_train_scaled = scaler.fit_transform(data_train)\n",
    "data_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = pd.DataFrame(data_train_scaled, columns=data_train.columns)\n",
    "data_train_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start\n",
    "print(\n",
    "    f\"The accuracy using a {model.__class__.__name__} is \"\n",
    "    f\"{model.score(data_test, target_test):.3f} with a fitting time of \"\n",
    "    f\"{elapsed_time:.3f} seconds in {model[-1].n_iter_} iterations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see that the training time and the number of iterations is much shorter\n",
    "while the predictive performance is equivalent.\n",
    "\n",
    "In the previous example, we split the original data into a training set and a\n",
    "testing set. This strategy has several issues: in the setting where the amount\n",
    "of data is limited, the subset of data used to train or test will be small;\n",
    "and the splitting was done in a random manner and we have no information\n",
    "regarding the confidence of the results obtained.\n",
    "\n",
    "Instead, we can use what cross-validation. Cross-validation consists in\n",
    "repeating this random splitting into training and testing sets and aggregate\n",
    "the model performance. By repeating the experiment, one can get an estimate of\n",
    "the variabilty of the model performance.\n",
    "\n",
    "The function `cross_val_score` allows for such experimental protocol by giving\n",
    "the model, the data and the target. Since there exists several\n",
    "cross-validation strategies, `cross_val_score` takes a parameter `cv` which\n",
    "defines the splitting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "score = cross_val_score(model, data_numeric, target, cv=5)\n",
    "print(f\"The accuracy (mean +/- 1 std. dev.) is: \"\n",
    "      f\"{score.mean():.3f} +/- {score.std():.3f}\")\n",
    "print(f\"The different scores obtained are: \\n{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Setting `cv=5` created 5 distinct splits to get 5 variations for the training\n",
    "and testing sets. Each training set is used to fit one model which is then\n",
    "scored on the matching test set. This strategy is called K-fold\n",
    "cross-validation where `K` corresponds to the number of splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO add CV diagram for 5-fold CV from the gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Work with categorical data\n",
    "\n",
    "In the previous section, we dealt with data for which numerical algorithms are\n",
    "mathematically designed to work natively. However, real datasets contain type\n",
    "of data which do not belong to this category and will require some\n",
    "preprocessing. Such preprocessing will transform these data to be numerical\n",
    "and thus natively handled by machine learning algorithms.\n",
    "\n",
    "Categorical data are broadly encountered in data science. Numerical data is a\n",
    "continuous quantity corresponding to a real numbers while categorical data are\n",
    "represented as discrete values. For instance, the variable `SEX` in our\n",
    "previous dataset is a categorical variable because it encodes the data with\n",
    "the two categories `male` and `female`.\n",
    "\n",
    "In the remainder of this section, we will present different strategies to\n",
    "encode categorical data into numerical data which can be used by a\n",
    "machine-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation', 'relationship',\n",
    "    'race', 'sex', 'native-country'\n",
    "]\n",
    "data_categorical = data[categorical_columns]\n",
    "data_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Encode categories having an ordering\n",
    "\n",
    "The most intuitive strategy is to encode each category by a numerical value.\n",
    "The `OrdinalEncoder` will transform the data in such manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "print(data_categorical.head())\n",
    "print(f\"The datasets is composed of {data_categorical.shape[1]} features\")\n",
    "encoder = OrdinalEncoder()\n",
    "data_encoded = encoder.fit_transform(data_categorical)\n",
    "\n",
    "print(f\"The dataset encoded contains {data_encoded.shape[1]} features\")\n",
    "print(data_encoded[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see that all categories have been encoded for each feature\n",
    "independently. We can also notice that the number of feature before and after\n",
    "the encoding is the same.\n",
    "\n",
    "However, one has to be careful when using this encoding strategy. The encoding\n",
    "imposed an order regarding the categories: 0 is smaller than 1 which is\n",
    "smaller than 2, etc. If the original categories did not have such order then\n",
    "this encoding is not adequate and you should use one-hot encoding instead.\n",
    "\n",
    "### Encode categories which do not have an ordering\n",
    "\n",
    "As previously stated, `OrdinalEncoder` is encoding categorical data having an\n",
    "ordering. In this case, the `OneHotEncoder` should be used. For a given\n",
    "feature, it will create as many new columns as categories. For a sample, the\n",
    "column corresponding to the category will be set to `1` while the other\n",
    "columns will be set to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print(data_categorical.head())\n",
    "print(f\"The datasets is composed of {data_categorical.shape[1]} features\")\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "data_encoded = encoder.fit_transform(data_categorical)\n",
    "\n",
    "print(f\"The dataset encoded contains {data_encoded.shape[1]} features\")\n",
    "print(data_encoded[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One can notice that the number of features after the encoding is larger than\n",
    "in the original data.\n",
    "\n",
    "Once the encoding is done, we could integrate it inside a machine learning\n",
    "pipeline as in the case with numerical data. In the following, we train a\n",
    "linear classifier on the encoded data and check the performance of this\n",
    "machine learning pipeline using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(OneHotEncoder(handle_unknown='ignore'),\n",
    "                      LogisticRegression(max_iter=1000))\n",
    "score = cross_val_score(model, data_categorical, target)\n",
    "print(f\"The accuracy is: {score.mean():.3f} +/- {score.std():.3f}\")\n",
    "print(f\"The different scores obtained are: \\n{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Combining different transformers used for different column types\n",
    "\n",
    "In the previous section, we saw that we need to treat data specifically\n",
    "depending of their nature (i.e. numerical or categorical) and trained one\n",
    "pipeline (transformer + classifier) for each kind of dataset.\n",
    "\n",
    "Scikit-learn provides a `ColumnTransformer` class which will dispatch some\n",
    "specific columns to a specific transformer making it easy to fit a single\n",
    "predictive model on a data that combines both kinds of variable together\n",
    "(heterogeneously typed tabular data).\n",
    "\n",
    "We can first define the columns depending on their data type:\n",
    "* **binary encoding** will be applied to categorical columns with only too\n",
    "  possible values (e.g. sex=male or sex=female in this example). Each binary\n",
    "  categorical columns will be mapped to one numerical columns with 0 or 1\n",
    "  values.\n",
    "* **one-hot encoding** will be applied to categorical columns with more that\n",
    "  two possible categories. This encoding will create one additional column for\n",
    "  each possible categorical value.\n",
    "* **numerical scaling** numerical features which will be standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_encoding_columns = ['sex']\n",
    "one_hot_encoding_columns = ['workclass', 'education', 'marital-status',\n",
    "                            'occupation', 'relationship',\n",
    "                            'race', 'native-country']\n",
    "scaling_columns = ['age', 'education-num', 'hours-per-week',\n",
    "                   'capital-gain', 'capital-loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now create our `ColumnTransfomer` by specifying a list of triplet\n",
    "(preprocessor name, transformer, columns). Finally, we can define a pipeline\n",
    "to stack this \"preprocessor\" with our classifier (logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('binary-encoder', OrdinalEncoder(), binary_encoding_columns),\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown='ignore'),\n",
    "     one_hot_encoding_columns),\n",
    "    ('standard-scaler', StandardScaler(), scaling_columns)\n",
    "])\n",
    "model = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The final model is more complex than the previous models but still follows the\n",
    "same API:\n",
    "- the `fit` method is called to preprocess the data then train the classifier;\n",
    "- the `predict` method can make predictions on new data;\n",
    "- the `score` method is used to predict on the test data and compare the\n",
    "  predictions to the expected test labels to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42\n",
    ")\n",
    "model.fit(data_train, target_train)\n",
    "model.predict(data_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The final can therefore be cross-validated as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score = cross_val_score(model, data, target, cv=5)\n",
    "print(f\"The accuracy is: {score.mean():.3f} +- {score.std():.3f}\")\n",
    "print(f\"The different scores obtained are: \\n{score}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "notebooks//ipynb,markdown_files//md,python_scripts//py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
